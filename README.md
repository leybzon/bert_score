
# BERTScore LLM Quality Evaluation

This repository contains scripts to evaluate the quality of responses generated by a Large Language Model (LLM) using BERTScore. The scripts can run in parallel or sequentially to process a batch of questions, compare the generated answers to reference answers, and generate a report with quality metrics (precision, recall, and F1).

## Table of Contents
- [Installation](#installation)
- [Scripts Overview](#scripts-overview)
- [Usage](#usage)
    - [evaluate_llm_quality.py](#evaluate_llm_qualitypy)
    - [evaluate_llm_quality_parallel.py](#evaluate_llm_quality_parallelpy)
- [Parameters](#parameters)
- [Example CSV Files](#example-csv-files)
- [License](#license)

---

## Installation

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/yourusername/bert_score.git
   cd bert_score
   

2. **Create a Virtual Environment (Optional but Recommended):**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   

3. **Install Required Dependencies:**
   ```bash
   pip install -r requirements.txt
   

### Dependencies
- requests: For sending requests to the vLLM server.
- bert-score: For calculating BERTScore between generated and reference answers.
- pandas: For reading and writing CSV files.
- tqdm: (for parallel script) To display progress bars during batch processing.
- concurrent.futures: (for parallel script) For running batch processing in parallel.



---

## Scripts Overview

This repository contains two main scripts:

1. **evaluate_llm_quality.py**: Processes questions sequentially and evaluates the generated answers using BERTScore.
2. **evaluate_llm_quality_parallel.py**: Processes questions in parallel using multiple threads, making it faster for large batches of questions.

Both scripts read from an input CSV containing questions and reference answers and generate an output CSV with evaluation metrics (precision, recall, and F1 scores).

---

## Usage

### **evaluate_llm_quality.py**

This script processes questions sequentially and evaluates the quality of the LLM responses using BERTScore.

**Usage:**
  python3 evaluate_llm_quality.py --input_csv questions.csv --output_csv report.csv


#### Optional Arguments:
- --input_csv: Path to the input CSV file (default: questions.csv).
- --output_csv: Path to the output CSV file (default: report.csv).
- --vllm_url: URL of the vLLM server API (default: http://localhost:8000/v1/chat/completions).
- --model_name: Name of the model to use for inference (default: mistralai/Mistral-7B-Instruct-v0.3).
- --temperature: Sampling temperature for inference (default: 0.7).
- --max_tokens: Maximum number of tokens to generate (default: 100).

**Example:**
   python3 evaluate_llm_quality.py --input_csv custom_questions.csv --output_csv results.csv --model_name gpt3 --temperature 0.9 --max_tokens 150


### **evaluate_llm_quality_parallel.py**

This script processes questions in parallel, leveraging multiple threads to speed up the batch evaluation.

**Usage:**
  python3 evaluate_llm_quality_parallel.py --input_csv questions.csv --output_csv report.csv --max_workers 10


#### Optional Arguments:
- --input_csv: Path to the input CSV file (default: questions.csv).
- --output_csv: Path to the output CSV file (default: report.csv).
- --vllm_url: URL of the vLLM server API (default: http://localhost:8000/v1/chat/completions).
- --model_name: Name of the model to use for inference (default: mistralai/Mistral-7B-Instruct-v0.3).
- --temperature: Sampling temperature for inference (default: 0.7).
- --max_tokens: Maximum number of tokens to generate (default: 100).
- --max_workers: Number of threads for parallel processing (default: 10).

**Example:**
python3 evaluate_llm_quality_parallel.py --input_csv custom_questions.csv --output_csv results.csv --model_name gpt3 --temperature 0.8 --max_tokens 200 --max_workers 20


---

## Parameters

| Parameter        | Description                                                              | Default Value                              |
|------------------|--------------------------------------------------------------------------|--------------------------------------------|
| --input_csv    | Path to the input CSV file containing questions and reference answers     | questions.csv                            |
| --output_csv   | Path to the output CSV file where the evaluation results will be saved    | report.csv                               |
| --vllm_url     | The URL of the vLLM server API endpoint                                   | http://localhost:8000/v1/chat/completions|
| --model_name   | The model name to use for inference                                       | mistralai/Mistral-7B-Instruct-v0.3       |
| --temperature  | Temperature for sampling during inference                                 | 0.7                                      |
| --max_tokens   | Maximum number of tokens to generate                                      | 100                                      |
| --max_workers  | (Parallel script) Number of parallel threads for batch processing         | 10                                       |

---

## Example CSV Files

The input CSV file should have the following format with three columns:
- question_id: A unique identifier for each question.
- question_text: The actual question being asked.
- reference_answer: The correct (reference) answer for comparison.

**Example (questions.csv):**
question_id,question_text,reference_answer
1,What is the capital of France?,The capital of France is Paris.
2,Who is the CEO of Microsoft?,The CEO of Microsoft is Satya Nadella.
3,What is the distance between the Earth and the Moon?,The average distance between the Earth and the Moon is approximately 384400 kilometers.


The output CSV file will have the following columns:
- question_id
- question_text
- generated_answer
- reference_answer
- precision
- recall
- f1




---

## License

See the [LICENSE](LICENSE) file for details.

---

